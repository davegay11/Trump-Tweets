# Trump's Tweets
## Group Members: 
    * Andrew Cho
    * Bryce Wolery
    * Christopher Sides
    * Dave Gay
    * Jeremy Fox
    * Sean Hudson
    * Zachary Marion

This is the official repository for The Trump's Tweets team in Professor Herron's Humanities: Data Mining + Meaning class at Duke University.

## Required packages

To install the required pip modules we recommend using virtualenv (can be installed with `pip install virtualenv`)

Initilaize your virtual environment with:

`virtualenv ~/.trump`

Source the environment with:

`source ~/.trump/bin/activate`

Next (in the repository root), install the packages necessary for this project with:

`pip install -r requirements.txt`

NOTE: Because matplotlib is a bit wierd, you need to make the file `~/.matplotlib/matplotlibrc` and in it put "backend: TkAgg". This can be done with:

`mkdir -p ~/.matplotlib; echo "backend: TkAgg" >> ~/.matplotlib/matplotlibrc`

Specifically from NLTK you need the vader lexicon and stopwords corpus. These can be installed through python using the command `nltk.download()` and following the GUI.

## Data Aquisition and Cleaning

To get the data, run the following script from the root project directory:

`./scripts/get_tweets.sh`

To clean the data and generate the models, run:

`./pipeline/main.py`

By default it cleans the data, generates word2vec models, and creates wordclouds for each individual. The file is formatted as a command line utility, whose options can be found with:

`/pipeline/main.py --help`

`fields.txt` contains an ascii table describing each field in the cleaned data and what values it can take on.

## File structure

The general file structure of the project is described below (some files ommitted).

```ruby
> data/
  |--> raw_json/             # The raw json pulled from the Trump Twitter Archive using ./scripts/get_tweets.sh
  |--> clean_data/           # The clean data that is generated from ./pipeline/main.py
  |--> tf_idf/               # Contains the closeness rankings of users to Trump from ./pipeline/main.py
  |--> segmented_tweets/     # Contains the segemented tweets from from /pipeline/pastTrump.py
> pipeline/
  |--> main.py               # The main script that parses, cleans, and generates models for each twitter corpus
> img/                       # Folder for all the images generated in the analysis
  |--> wordclouds/           # The wordclouds for each user generated from ./pipeline/main.py
> models/
  |--> word2vec/             # Binary files containing the word2vec encodings of user, from ./pipeline/main.py
> scripts/                   # Bash scripts associated with the project
  |--> get_tweets.sh         # Script to pull tweets from the Trump Twitter Archive
> tmp/                       # Place for temporary files generated by various scripts
> saved_ngrams/              # Location of ngrams generated for Trump during modelling
```

Additonally there are several Jupyter notebooks in the root directory whose names correspond to the data explorations they contain.

## Licence

This data is made available under the Public Domain Dedication and License version v1.0 whose full text can be found at http://opendatacommons.org/licenses/pddl/ or located in license.txt within this repository.
