{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# In this file we make a bag-of-words representation of the clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./clean_data/clean_tweets.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        trump international tower chicago ranked th ta...\n",
       "1                     wishing happy bountiful thanksgiving\n",
       "2        donald trump partners tv new reality series en...\n",
       "3        hear donald trump discuss big gov spending ban...\n",
       "4        watch video ivanka trump sharing business advi...\n",
       "5        read donald trump say daughter ivanka upcoming...\n",
       "6        lot people imagination execute execute imagina...\n",
       "7                   read donald trump top ten tips success\n",
       "8        hysterical dsrl videos featuring donald trump ...\n",
       "9        donald trump bids buy oreo double stuf racing ...\n",
       "10       reminder miss universe competition live bahama...\n",
       "11       watch miss universe competition live bahamas s...\n",
       "12       watch donald trump recent appearance late show...\n",
       "13       ivanka twitter follow @ivankatrump terrific we...\n",
       "14       browse donald trump summer reading list busine...\n",
       "15       check list donald trump books summer reading t...\n",
       "16       congrats winners around world entered think li...\n",
       "17       donald trump backs apprentice randal pinkett n...\n",
       "18       aware things seem inexplicable big step toward...\n",
       "19       safe happy independence day one enjoy donald j...\n",
       "20       watch powerful frank interview donald trump ec...\n",
       "21       wishing happy father day dad champion today ev...\n",
       "22       fb vanity urls sf chronicle david beckham one ...\n",
       "23           today donald trump birthday send b day wishes\n",
       "24       last week enter think like champion signed boo...\n",
       "25            check donald trump new igoogle showcase page\n",
       "26       know call quits keep moving forward donald j t...\n",
       "27         read excerpt think like champion donald j trump\n",
       "28       higher self direct opposition comfort zone don...\n",
       "29             know donald trump facebook become fan today\n",
       "                               ...                        \n",
       "26715    time republicans democrats get together come h...\n",
       "26716    typical political thing blame fact obamacare l...\n",
       "26717    democrats lead head clown chuck schumer know b...\n",
       "26718    jackie evancho album sales skyrocketed announc...\n",
       "26719    massive increases obamacare take place year de...\n",
       "26720    like hike arizona also deductibles high practi...\n",
       "26721    republicans must careful dems failed obamacare...\n",
       "26722    things said like giving questions debate h tot...\n",
       "26723    somebody hacked dnc hacking defense like rnc r...\n",
       "26724    thank ford scrapping new plant mexico creating...\n",
       "26725    julian assange said year old could hacked pode...\n",
       "26726    intelligence briefing called russian hacking d...\n",
       "26727    general news conference january eleventh n c t...\n",
       "26728       trump already delivering jobs promised america\n",
       "26729    releases gitmo extremely dangerous people allo...\n",
       "26730    instead driving jobs wealth away america becom...\n",
       "26731    @danscavino ford scrap mexico plant invest mic...\n",
       "26732    may number one act priority focus tax reform h...\n",
       "26733    congress work really make weakening independen...\n",
       "26734    democrat governor minnesota said affordable ca...\n",
       "26735    people must remember obamacare work affordable...\n",
       "26736    general motors sending mexican made model chev...\n",
       "26737    china taking massive amounts money wealth u to...\n",
       "26738    north korea stated final stages developing nuc...\n",
       "26739    thought felt would win big easily fabled cance...\n",
       "26740    various media outlets pundits say thought goin...\n",
       "26741    @cnn released book called unprecedented explor...\n",
       "26742    chicago murder rate record setting shooting vi...\n",
       "26743     well new year begins together make america great\n",
       "26744    rt @donaldjtrumpjr happy new year everyone new...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 05:44:20,238 : INFO : collecting all words and their counts\n",
      "2017-03-21 05:44:20,240 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-03-21 05:44:20,249 : INFO : collected 5196 word types from a corpus of 27301 raw words and 2795 sentences\n",
      "2017-03-21 05:44:20,251 : INFO : Loading a fresh vocabulary\n",
      "2017-03-21 05:44:20,261 : INFO : min_count=10 retains 562 unique words (10% of original 5196, drops 4634)\n",
      "2017-03-21 05:44:20,263 : INFO : min_count=10 leaves 17339 word corpus (63% of original 27301, drops 9962)\n",
      "2017-03-21 05:44:20,267 : INFO : deleting the raw counts dictionary of 5196 items\n",
      "2017-03-21 05:44:20,269 : INFO : sample=0.001 downsamples 85 most-common words\n",
      "2017-03-21 05:44:20,270 : INFO : downsampling leaves estimated 13970 word corpus (80.6% of prior 17339)\n",
      "2017-03-21 05:44:20,272 : INFO : estimated required memory for 562 words and 300 dimensions: 1629800 bytes\n",
      "2017-03-21 05:44:20,275 : INFO : resetting layer weights\n",
      "2017-03-21 05:44:20,288 : INFO : training model with 4 workers on 562 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-03-21 05:44:20,290 : INFO : expecting 2795 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-03-21 05:44:20,417 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-03-21 05:44:20,426 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-03-21 05:44:20,431 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 05:44:20,437 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-03-21 05:44:20,438 : INFO : training on 136505 raw words (69823 effective words) took 0.1s, 486677 effective words/s\n",
      "2017-03-21 05:44:20,440 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-03-21 05:44:20,443 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-03-21 05:44:20,453 : INFO : saving Word2Vec object under trump2vec.bin, separately None\n",
      "2017-03-21 05:44:20,457 : INFO : not storing attribute syn0norm\n",
      "2017-03-21 05:44:20,460 : INFO : not storing attribute cum_table\n",
      "2017-03-21 05:44:20,473 : INFO : saved trump2vec.bin\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(\n",
    "            [s.split() for s in list(df['clean_text'])], \n",
    "            workers=num_workers, size=num_features, \\\n",
    "            min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# calling init_sims make the training more efficient if we don't plan on\n",
    "# training the model any further\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"trump2vec.bin\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crooked', 0.9999712705612183),\n",
       " ('u', 0.9999687075614929),\n",
       " ('bad', 0.9999686479568481),\n",
       " ('people', 0.9999662041664124),\n",
       " ('one', 0.999965488910675),\n",
       " ('president', 0.9999635815620422),\n",
       " ('never', 0.9999634027481079),\n",
       " ('many', 0.999963104724884),\n",
       " ('would', 0.9999619722366333),\n",
       " ('big', 0.9999618530273438)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cool! Now the model is trained...let's have some fun\n",
    "model.most_similar(\"hillary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 0.9999418258666992),\n",
       " ('u', 0.9999414682388306),\n",
       " ('one', 0.9999412298202515),\n",
       " ('many', 0.9999408721923828),\n",
       " ('last', 0.999938428401947),\n",
       " ('obama', 0.9999377131462097),\n",
       " ('people', 0.9999376535415649),\n",
       " ('bad', 0.9999370574951172),\n",
       " ('must', 0.9999368786811829),\n",
       " ('crooked', 0.9999368190765381)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('u', 0.9999639391899109),\n",
       " ('big', 0.999962568283081),\n",
       " ('many', 0.9999623894691467),\n",
       " ('great', 0.9999606013298035),\n",
       " ('people', 0.9999601244926453),\n",
       " ('trump', 0.9999594688415527),\n",
       " ('crooked', 0.9999594688415527),\n",
       " ('one', 0.9999594688415527),\n",
       " ('media', 0.9999585151672363),\n",
       " ('much', 0.999958336353302)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>truncated</th>\n",
       "      <th>text</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>id</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>source</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>lang</th>\n",
       "      <th>created_at</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16553</th>\n",
       "      <td>False</td>\n",
       "      <td>https://t.co/T5JBFXOz3F</td>\n",
       "      <td>False</td>\n",
       "      <td>669980142475845632</td>\n",
       "      <td>4481</td>\n",
       "      <td>iphone</td>\n",
       "      <td>False</td>\n",
       "      <td>1989</td>\n",
       "      <td>False</td>\n",
       "      <td>und</td>\n",
       "      <td>Thu Nov 26 20:44:48 +0000 2015</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18901</th>\n",
       "      <td>False</td>\n",
       "      <td>http://t.co/PtViAyrO4A</td>\n",
       "      <td>False</td>\n",
       "      <td>619646907468632064</td>\n",
       "      <td>4345</td>\n",
       "      <td>iphone</td>\n",
       "      <td>False</td>\n",
       "      <td>3027</td>\n",
       "      <td>False</td>\n",
       "      <td>und</td>\n",
       "      <td>Fri Jul 10 23:18:29 +0000 2015</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19450</th>\n",
       "      <td>False</td>\n",
       "      <td>There is. https://t.co/nCOUYoClDN</td>\n",
       "      <td>True</td>\n",
       "      <td>609388571951239168</td>\n",
       "      <td>47</td>\n",
       "      <td>web</td>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Fri Jun 12 15:55:32 +0000 2015</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25306</th>\n",
       "      <td>False</td>\n",
       "      <td>https://t.co/ZQ0osiFEJQ</td>\n",
       "      <td>False</td>\n",
       "      <td>708351381678088192</td>\n",
       "      <td>10424</td>\n",
       "      <td>iphone</td>\n",
       "      <td>False</td>\n",
       "      <td>4923</td>\n",
       "      <td>False</td>\n",
       "      <td>und</td>\n",
       "      <td>Fri Mar 11 17:58:24 +0000 2016</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26004</th>\n",
       "      <td>False</td>\n",
       "      <td>https://t.co/SmTkLPiBYD</td>\n",
       "      <td>False</td>\n",
       "      <td>692171744845664258</td>\n",
       "      <td>18204</td>\n",
       "      <td>iphone</td>\n",
       "      <td>False</td>\n",
       "      <td>8278</td>\n",
       "      <td>False</td>\n",
       "      <td>und</td>\n",
       "      <td>Wed Jan 27 02:26:18 +0000 2016</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      truncated                               text is_quote_status  \\\n",
       "16553     False            https://t.co/T5JBFXOz3F           False   \n",
       "18901     False             http://t.co/PtViAyrO4A           False   \n",
       "19450     False  There is. https://t.co/nCOUYoClDN            True   \n",
       "25306     False            https://t.co/ZQ0osiFEJQ           False   \n",
       "26004     False            https://t.co/SmTkLPiBYD           False   \n",
       "\n",
       "                       id  favorite_count  source retweeted  retweet_count  \\\n",
       "16553  669980142475845632            4481  iphone     False           1989   \n",
       "18901  619646907468632064            4345  iphone     False           3027   \n",
       "19450  609388571951239168              47     web     False             17   \n",
       "25306  708351381678088192           10424  iphone     False           4923   \n",
       "26004  692171744845664258           18204  iphone     False           8278   \n",
       "\n",
       "      favorited lang                      created_at clean_text  \n",
       "16553     False  und  Thu Nov 26 20:44:48 +0000 2015        NaN  \n",
       "18901     False  und  Fri Jul 10 23:18:29 +0000 2015        NaN  \n",
       "19450     False   en  Fri Jun 12 15:55:32 +0000 2015        NaN  \n",
       "25306     False  und  Fri Mar 11 17:58:24 +0000 2016        NaN  \n",
       "26004     False  und  Wed Jan 27 02:26:18 +0000 2016        NaN  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
