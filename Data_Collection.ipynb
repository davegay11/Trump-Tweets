{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import json\n",
    "import tweepy\n",
    "\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "access_key = \"85439021-G7lAa4TJiW2cuUOte8Axrjfyc8vEIb9d0tMfnu9Ut\"\n",
    "access_secret = \"93ez5xeUEMgJbfxq0RTUlF5Jm9eWeX52JsTq8crXyBwZh\"\n",
    "consumer_key = \"jzIKubaRqBQfsqdNAu0wruaha\"\n",
    "consumer_secret = \"nob4g5PE5K8SLm7jaIx1VQfAa77DLP4RPbFQAj50A8fNJ7hBW7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that collects tweets via Twtiter API\n",
    "\n",
    "def get_all_tweets(screen_name):\n",
    " \n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    \n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []    \n",
    "    \n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
    "    \n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "    \n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "    \n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        \n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=500,max_id=oldest)\n",
    "        \n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "        \n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        print \"...%s tweets downloaded so far\" % (len(alltweets))\n",
    "    \n",
    "    #write tweet objects to JSON\n",
    "    \n",
    "    with open(screen_name+'.json', 'w') as outfile:\n",
    "        json.dump([status._json for status in alltweets], outfile)\n",
    "    print \"Done\"\n",
    "    \n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...400 tweets downloaded so far\n",
      "...600 tweets downloaded so far\n",
      "...799 tweets downloaded so far\n",
      "...999 tweets downloaded so far\n",
      "...1199 tweets downloaded so far\n",
      "...1399 tweets downloaded so far\n",
      "...1599 tweets downloaded so far\n",
      "...1799 tweets downloaded so far\n",
      "...1999 tweets downloaded so far\n",
      "...2199 tweets downloaded so far\n",
      "...2399 tweets downloaded so far\n",
      "...2599 tweets downloaded so far\n",
      "...2799 tweets downloaded so far\n",
      "...2999 tweets downloaded so far\n",
      "...3199 tweets downloaded so far\n",
      "...3230 tweets downloaded so far\n",
      "...3230 tweets downloaded so far\n",
      "Done\n",
      "...400 tweets downloaded so far\n",
      "...600 tweets downloaded so far\n",
      "...800 tweets downloaded so far\n",
      "...1000 tweets downloaded so far\n",
      "...1200 tweets downloaded so far\n",
      "...1400 tweets downloaded so far\n",
      "...1600 tweets downloaded so far\n",
      "...1800 tweets downloaded so far\n",
      "...2000 tweets downloaded so far\n",
      "...2200 tweets downloaded so far\n",
      "...2400 tweets downloaded so far\n",
      "...2600 tweets downloaded so far\n",
      "...2800 tweets downloaded so far\n",
      "...3000 tweets downloaded so far\n",
      "...3200 tweets downloaded so far\n",
      "...3232 tweets downloaded so far\n",
      "...3232 tweets downloaded so far\n",
      "Done\n",
      "...400 tweets downloaded so far\n",
      "...600 tweets downloaded so far\n",
      "...800 tweets downloaded so far\n",
      "...1000 tweets downloaded so far\n",
      "...1200 tweets downloaded so far\n",
      "...1399 tweets downloaded so far\n",
      "...1599 tweets downloaded so far\n",
      "...1799 tweets downloaded so far\n",
      "...1999 tweets downloaded so far\n",
      "...2199 tweets downloaded so far\n",
      "...2399 tweets downloaded so far\n",
      "...2599 tweets downloaded so far\n",
      "...2799 tweets downloaded so far\n",
      "...2999 tweets downloaded so far\n",
      "...3199 tweets downloaded so far\n",
      "...3244 tweets downloaded so far\n",
      "...3244 tweets downloaded so far\n",
      "Done\n",
      "...397 tweets downloaded so far\n",
      "...597 tweets downloaded so far\n",
      "...797 tweets downloaded so far\n",
      "...997 tweets downloaded so far\n",
      "...1197 tweets downloaded so far\n",
      "...1397 tweets downloaded so far\n",
      "...1597 tweets downloaded so far\n",
      "...1797 tweets downloaded so far\n",
      "...1997 tweets downloaded so far\n",
      "...2194 tweets downloaded so far\n",
      "...2393 tweets downloaded so far\n",
      "...2593 tweets downloaded so far\n",
      "...2792 tweets downloaded so far\n",
      "...2992 tweets downloaded so far\n",
      "...3192 tweets downloaded so far\n",
      "...3197 tweets downloaded so far\n",
      "...3197 tweets downloaded so far\n",
      "Done\n",
      "...4 tweets downloaded so far\n",
      "Done\n",
      "...400 tweets downloaded so far\n",
      "...600 tweets downloaded so far\n",
      "...800 tweets downloaded so far\n",
      "...999 tweets downloaded so far\n",
      "...1199 tweets downloaded so far\n",
      "...1398 tweets downloaded so far\n",
      "...1598 tweets downloaded so far\n",
      "...1798 tweets downloaded so far\n",
      "...1998 tweets downloaded so far\n",
      "...2198 tweets downloaded so far\n",
      "...2397 tweets downloaded so far\n",
      "...2594 tweets downloaded so far\n",
      "...2794 tweets downloaded so far\n",
      "...2993 tweets downloaded so far\n",
      "...3192 tweets downloaded so far\n",
      "...3199 tweets downloaded so far\n",
      "...3199 tweets downloaded so far\n",
      "Done\n",
      "...392 tweets downloaded so far\n",
      "...590 tweets downloaded so far\n",
      "...755 tweets downloaded so far\n",
      "...755 tweets downloaded so far\n",
      "Done\n",
      "...394 tweets downloaded so far\n",
      "...591 tweets downloaded so far\n",
      "...790 tweets downloaded so far\n",
      "...989 tweets downloaded so far\n",
      "...1188 tweets downloaded so far\n",
      "...1386 tweets downloaded so far\n",
      "...1585 tweets downloaded so far\n",
      "...1782 tweets downloaded so far\n",
      "...1980 tweets downloaded so far\n",
      "...2178 tweets downloaded so far\n",
      "...2377 tweets downloaded so far\n",
      "...2575 tweets downloaded so far\n",
      "...2774 tweets downloaded so far\n",
      "...2887 tweets downloaded so far\n",
      "...2887 tweets downloaded so far\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Request @realDonaldTrump here. Running this will save a JSON file at file location\n",
    "if __name__ == '__main__':\n",
    "    #pass in the username of the account you want to download\n",
    "    get_all_tweets(\"@realDonaldTrump\")\n",
    "    get_all_tweets(\"@HillaryClinton\")\n",
    "    get_all_tweets(\"@BernieSanders\")\n",
    "    get_all_tweets(\"@Schwarzenegger\")\n",
    "    get_all_tweets(\"@SteveKBannon\")\n",
    "    get_all_tweets(\"@GovGaryJohnson\")\n",
    "    get_all_tweets(\"@HarambesSpirit\")\n",
    "    get_all_tweets(\"@deeznutsHC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function that collects tweets via Twtiter API\n",
    "def get_raw_data(filename):\n",
    "    '''Get the raw json data downloaded from the tweepy API'''\n",
    "    data = None\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def get_baseline():\n",
    " \n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    \n",
    "    #Search token for tweets in the US\n",
    "    searchQuery = 'place:96683cc9126741d1'\n",
    "    #Maximum number of tweets we want to collect per run\n",
    "    maxTweets = 1000\n",
    "    tweetsPerQry = 100\n",
    "    \n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = []\n",
    "    \n",
    "    #load old tweets\n",
    "    oldData = False\n",
    "    try:\n",
    "        oldTweets = get_raw_data(\"baseline_tweets.json\")\n",
    "        oldData = True\n",
    "    except:\n",
    "        print  \"No Old Data\"\n",
    "    \n",
    "    tweetCount = 0\n",
    "    \n",
    "    for tweet in tweepy.Cursor(api.search,q=searchQuery).items(maxTweets):         \n",
    "        #Verify the tweet has place info before writing (It should, if it got past our place filter)\n",
    "        if tweet.place is not None:\n",
    "            #Write the JSON format to the text file, and add one to the number of tweets we've collected\n",
    "            new_tweets.append(tweet)\n",
    "            tweetCount += 1\n",
    "\n",
    "    #Display how many tweets we have collected\n",
    "    print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "    \n",
    "    #write tweet objects to JSON\n",
    "    if oldData:\n",
    "        oldTweets.extend([status._json for status in new_tweets])\n",
    "    \n",
    "    with open('baseline_tweets.json', 'w') as outfile:\n",
    "        if oldData:\n",
    "            json.dump(oldTweets, outfile)\n",
    "        else:\n",
    "            json.dump([status._json for status in new_tweets], outfile)\n",
    "    print \"Done\"\n",
    "    \n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 774 tweets\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "get_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline = get_raw_data('baseline_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2340"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
